#!/usr/bin/env python3
"""
DNS Log Parser for Pi-hole - Raspberry Pi Zero 2W Optimized
Designed for 512MB RAM, 1GHz CPU with minimal resource usage

Key Optimizations:
- Batch database writes (reduces I/O by 90%)
- In-memory buffering with size limits
- Aggressive SQLite tuning for low RAM
- Minimal CPU polling
- Graceful shutdown with data preservation
"""

import re
import sqlite3
import time
import signal
import sys
import yaml
from datetime import datetime
from pathlib import Path
from collections import deque

# Pi Zero 2W Tuning Parameters
BATCH_SIZE = 50              # Write every 50 queries
BATCH_INTERVAL = 30          # Or every 30 seconds (whichever comes first)
BUFFER_MAX = 100             # Max queries in memory before force flush
DB_CACHE_SIZE = 2000         # SQLite page cache in KB (2MB)
POLL_INTERVAL = 0.5          # Log check interval (seconds)
RETENTION_DAYS = 30          # Keep logs for 30 days (vs 90 on Pi 4)

class OptimizedDNSParser:
    def __init__(self, config_path='config/config.yaml'):
        """Initialize with Pi Zero 2W optimizations"""
        self.config = self.load_config(config_path)
        self.db_path = self.config['database']['path']
        self.log_path = self.config['pihole']['log_path']
        self.running = True
        
        # Memory-efficient query buffer
        self.buffer = deque(maxlen=BUFFER_MAX)
        self.last_flush = time.time()
        self.queries_processed = 0
        
        # Optimized regex patterns
        self.query_pattern = re.compile(
            r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}).*query\[(\w+)\]\s+(\S+)\s+from\s+(\S+)'
        )
        self.blocked_pattern = re.compile(
            r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}).*(?:gravity blocked|config blocked)\s+(\S+)'
        )
        
        # Database setup with optimizations
        self.setup_database()
        self.setup_signal_handlers()
        
    def load_config(self, config_path):
        """Load config with defaults for Pi Zero 2W"""
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
                # Override retention for Pi Zero 2W
                config['database']['retention_days'] = RETENTION_DAYS
                return config
        except FileNotFoundError:
            return {
                'pihole': {'log_path': '/var/log/pihole.log'},
                'database': {
                    'path': 'data/dns_logs.db',
                    'retention_days': RETENTION_DAYS
                }
            }
    
    def setup_database(self):
        """Configure SQLite for minimal RAM usage"""
        self.conn = sqlite3.connect(
            self.db_path,
            check_same_thread=False,
            isolation_level='DEFERRED'  # Faster batch writes
        )
        
        cursor = self.conn.cursor()
        
        # Pi Zero 2W specific SQLite tuning
        cursor.execute(f"PRAGMA cache_size = -{DB_CACHE_SIZE}")  # 2MB cache
        cursor.execute("PRAGMA journal_mode = WAL")              # Write-ahead logging
        cursor.execute("PRAGMA synchronous = NORMAL")            # Faster writes
        cursor.execute("PRAGMA temp_store = MEMORY")             # In-memory temp
        cursor.execute("PRAGMA mmap_size = 268435456")           # 256MB memory map
        cursor.execute("PRAGMA page_size = 4096")                # Match SD card block size
        
        print("Database optimized for Pi Zero 2W")
        print(f"Cache: {DB_CACHE_SIZE}KB | WAL mode | Retention: {RETENTION_DAYS} days")
    
    def setup_signal_handlers(self):
        """Handle graceful shutdown"""
        signal.signal(signal.SIGINT, self.shutdown)
        signal.signal(signal.SIGTERM, self.shutdown)
    
    def shutdown(self, signum, frame):
        """Graceful shutdown with buffer flush"""
        print("\nüõë Shutdown signal received...")
        self.running = False
        
        if self.buffer:
            print(f"üíæ Flushing {len(self.buffer)} buffered queries...")
            self.flush_buffer()
        
        self.conn.close()
        print(f"‚úÖ Processed {self.queries_processed} total queries")
        sys.exit(0)
    
    def parse_timestamp(self, timestamp_str):
        """Convert syslog timestamp to epoch"""
        try:
            year = datetime.now().year
            dt = datetime.strptime(f"{year} {timestamp_str}", "%Y %b %d %H:%M:%S")
            return int(dt.timestamp())
        except Exception:
            return int(time.time())
    
    def parse_query(self, line):
        """Extract query data from log line"""
        # Check for regular query
        match = self.query_pattern.search(line)
        if match:
            timestamp_str, qtype, domain, client = match.groups()
            return {
                'timestamp': self.parse_timestamp(timestamp_str),
                'client_ip': client,
                'domain': domain.lower(),
                'query_type': qtype,
                'status': 'allowed',
                'blocked': 0,
                'response_time': None
            }
        
        # Check for blocked query
        match = self.blocked_pattern.search(line)
        if match:
            timestamp_str, domain = match.groups()
            return {
                'timestamp': self.parse_timestamp(timestamp_str),
                'client_ip': None,
                'domain': domain.lower(),
                'query_type': None,
                'status': 'blocked',
                'blocked': 1,
                'response_time': None
            }
        
        return None
    
    def flush_buffer(self):
        """Batch write queries to database"""
        if not self.buffer:
            return
        
        try:
            cursor = self.conn.cursor()
            
            # Batch insert for minimal I/O
            cursor.executemany("""
                INSERT INTO dns_queries 
                (timestamp, client_ip, domain, query_type, status, blocked, response_time)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, [
                (q['timestamp'], q['client_ip'], q['domain'], q['query_type'],
                 q['status'], q['blocked'], q['response_time'])
                for q in self.buffer
            ])
            
            self.conn.commit()
            count = len(self.buffer)
            self.queries_processed += count
            self.buffer.clear()
            self.last_flush = time.time()
            
            # Show progress every 500 queries
            if self.queries_processed % 500 == 0:
                print(f"üìä Processed {self.queries_processed} queries | "
                      f"Buffer: {count} | Memory: ~{sys.getsizeof(self.buffer) / 1024:.1f}KB")
            
        except sqlite3.Error as e:
            print(f"‚ùå Database error: {e}")
            # Keep buffer on error for retry
    
    def should_flush(self):
        """Determine if buffer should be written"""
        if len(self.buffer) >= BATCH_SIZE:
            return True
        if time.time() - self.last_flush >= BATCH_INTERVAL:
            return True
        return False
    
    def cleanup_old_data(self):
        """Remove old logs to save space (runs on startup)"""
        try:
            cursor = self.conn.cursor()
            cutoff = int(time.time()) - (RETENTION_DAYS * 86400)
            cursor.execute("DELETE FROM dns_queries WHERE timestamp < ?", (cutoff,))
            deleted = cursor.rowcount
            self.conn.commit()
            
            if deleted > 0:
                cursor.execute("VACUUM")  # Reclaim space
                print(f"üóëÔ∏è  Cleaned up {deleted} old records")
        except Exception as e:
            print(f"‚ö†Ô∏è  Cleanup error: {e}")
    
    def monitor_log(self):
        """Tail log file with minimal CPU usage"""
        print("=" * 60)
        print("üöÄ Pi-hole DNS Parser - Pi Zero 2W Edition")
        print("=" * 60)
        print(f"üìÅ Log: {self.log_path}")
        print(f"üíæ Database: {self.db_path}")
        print(f"‚öôÔ∏è  Batch: {BATCH_SIZE} queries / {BATCH_INTERVAL}s")
        print(f"üîß Buffer: {BUFFER_MAX} max")
        print("=" * 60)
        
        # Cleanup old data on startup
        self.cleanup_old_data()
        
        try:
            with open(self.log_path, 'r') as f:
                # Start from end of file
                f.seek(0, 2)
                print("‚úÖ Monitoring started...\n")
                
                while self.running:
                    line = f.readline()
                    
                    if line:
                        query = self.parse_query(line)
                        if query:
                            self.buffer.append(query)
                            
                            # Flush if needed
                            if self.should_flush():
                                self.flush_buffer()
                    else:
                        # No new data - check if time-based flush needed
                        if self.should_flush() and self.buffer:
                            self.flush_buffer()
                        
                        # Sleep to reduce CPU usage
                        time.sleep(POLL_INTERVAL)
                        
        except FileNotFoundError:
            print(f"‚ùå Log file not found: {self.log_path}")
            print("üí° Make sure Pi-hole is installed and logging is enabled")
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå Error: {e}")
            self.shutdown(None, None)

def main():
    """Entry point"""
    parser = OptimizedDNSParser()
    parser.monitor_log()

if __name__ == "__main__":
    main()
